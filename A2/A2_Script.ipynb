{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayamarshel/CART498/blob/main/A2_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "## P+7 (Oulipian language modelling)\n",
        "\n",
        "### Background\n",
        "\n",
        "The Oulipo (*Ouvroir de Littérature Potentielle*, or “Workshop of Potential Literature”) is a French literary group founded in 1960 by writer Raymond Queneau and mathematician François Le Lionnais. The group focuses on using rules and constraints in writing as a way to spark creativity. Rather than seeing constraints as obstacles, Oulipians treat them as tools to inspire new forms of storytelling and poetry. Their work combines mathematics, language, and playfulness, making their approach both unique and influential in modern literature.\n",
        "\n",
        "One of the most famous Oulipian writers is Georges Perec, who is known for his creative use of constraints. His novel *La Disparition* (“A Void”) is written entirely without the letter \"e,\" which is especially challenging given how common \"e\" is in French. Perec’s writing often plays with the structure of language in surprising ways. One popular Oulipian technique is N+7, where each noun in a text is replaced by the noun seven entries later in a dictionary. This creates unusual, absurd, and often funny results, encouraging writers to think differently about language and meaning.\n",
        "\n",
        "![George Perec](https://upload.wikimedia.org/wikipedia/commons/7/76/Myart_georges-perec_1978.jpg)\n",
        "\n",
        "(George Perec, 1978. From Wikidata)\n",
        "\n",
        "<!-- <img src=\"https://media.vigliensoni.com/clips/CART498/perec-01.jpg\" width=\"800\"> -->\n",
        "\n",
        "\n",
        "## How it works\n",
        "\n",
        "The N+7 technique process is straightforward\n",
        "\n",
        "- **Start with a text**. Choose any text—this could be a poem, a sentence, or a passage.\n",
        "- **Use a dictionary**. Have a dictionary (or word list) handy.\n",
        "- **Replace each noun**. For every substantive noun in the original text, replace it with the noun appearing seven nouns away in the dictionary. If the end of the dictionary is reached, you can loop back to the beginning.\n",
        "- **Maintain grammar**. Ensure the new text maintains grammatical correctness as much as possible, though the results often turn out  surreal and nonsensical.\n",
        "\n",
        "For example, using the N+7 technique with a standard English dictionary for the original sentence:\n",
        "\n",
        "*The cat sat on the mat*.\n",
        "\n",
        "- ”Cat” → 7 nouns after “cat” is “catalog.”\n",
        "- ”Mat” → 7 nouns after ”mat” is ”material.”\n",
        "\n",
        "Results in:\n",
        "\n",
        "*The catalog sat on the material.*\n",
        "\n",
        "### Assignment and deliverables\n",
        "\n",
        "For this assignment, you will create a variation of the N+7 technique we will name P+7. Using the GPT-2 language model, you will replace the last word of each line from *The Snow Man* with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "\n",
        "By the end of this assignment, submit a link to a GitHub repository named `CART498-GenAI` containing a folder labelled `A02` with the following items:\n",
        "\n",
        "- A version of the text processed with your P+7 technique, saved as a `.txt` file.\n",
        "- A second version of the text processed with `P+x`. Choose an `x` value that produces the funniest, wittiest, or most absurd version of the original text. Save this as a .txt file, and include the `x` value in the filename (e.g., `P+23.txt` or `P+12.txt`).\n",
        "- A Python notebook with the script used to generate your P+7 and P+x transformations using the GPT-2 language model.\n",
        "- A short reflection (250–350 words) explaining how altering the `x` value impacted the output of your P+x version. Additionally, discuss how you would implement a P+7 technique in which all nouns are replaced with their seventh-highest probability alternatives.\n",
        "\n",
        "### *The Snow Man*\n",
        "by Wallace Stevens (1879-1955)\n",
        "\n",
        "\n",
        "> One must have a mind of winter  \n",
        "> To regard the frost and the boughs  \n",
        "> Of the pine-trees crusted with snow;  \n",
        "> And have been cold a long time  \n",
        "> To behold the junipers shagged with ice,  \n",
        "> The spruces rough in the distant glitter  \n",
        "> Of the January sun; and not to think  \n",
        "> Of any misery in the sound of the wind,  \n",
        "> In the sound of a few leaves,  \n",
        "> Which is the sound of the land  \n",
        "> Full of the same wind  \n",
        "> That is blowing in the same bare place  \n",
        "> For the listener, who listens in the snow,  \n",
        "> And, nothing himself, beholds  \n",
        "> Nothing that is not there and the nothing that is.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lmvekj8ro3HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a prompt that takes a series of sentences separated by a dash - and then removes the last word from the sentence\n",
        "# puts them into an array called sentences. then print the array\n",
        "prompt = \"One must have a mind of winter- To regard the frost and the boughs- Of the pine-trees crusted with snow;- And have been cold a long time- To behold the junipers shagged with ice,-- The spruces rough in the distant glitter- Of the January sun; and not to think- Of any misery in the sound of the wind,- In the sound of a few leaves,- Which is the sound of the land- Full of the same wind- That is blowing in the same bare place- For the listener, who listens in the snow,- And, nothing himself, beholds- Nothing that is not there and the nothing that is.\"\n",
        "\n",
        "def process_prompt(prompt):\n",
        "    sentences = []\n",
        "    for sentence in prompt.split('-'):\n",
        "        words = sentence.strip().split()\n",
        "        if words:\n",
        "            sentences.append(\" \".join(words[:-1]))  # Remove the last word\n",
        "    return sentences\n",
        "\n",
        "prompt = \"One must have a mind of winter- To regard the frost and the boughs- Of the pine trees crusted with snow;- And have been cold a long time- To behold the junipers shagged with ice,-- The spruces rough in the distant glitter- Of the January sun; and not to think- Of any misery in the sound of the wind,- In the sound of a few leaves,- Which is the sound of the land- Full of the same wind- That is blowing in the same bare place- For the listener, who listens in the snow,- And, nothing himself, beholds- Nothing that is not there and the nothing that is.\"\n",
        "\n",
        "sentences = process_prompt(prompt)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlMcr0qI99n-",
        "outputId": "1bf006db-89f5-435c-ba57-be2d119eb6db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One must have a mind of',\n",
              " 'To regard the frost and the',\n",
              " 'Of the pine trees crusted with',\n",
              " 'And have been cold a long',\n",
              " 'To behold the junipers shagged with',\n",
              " 'The spruces rough in the distant',\n",
              " 'Of the January sun; and not to',\n",
              " 'Of any misery in the sound of the',\n",
              " 'In the sound of a few',\n",
              " 'Which is the sound of the',\n",
              " 'Full of the same',\n",
              " 'That is blowing in the same bare',\n",
              " 'For the listener, who listens in the',\n",
              " 'And, nothing himself,',\n",
              " 'Nothing that is not there and the nothing that']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now take the array of sentences and for each item in the array use the gpt 2 model to generate the 7 most likely words to come after the sentence and add them to an array of words that is titled with the sentence number they correspond to\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # or a specific GPT-2 variant\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "def p_plus_7(sentences):\n",
        "    results = {}\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Generate text using GPT-2\n",
        "        generated_text = generator(sentence, max_length=len(sentence.split()) + 7, num_return_sequences=1)\n",
        "        generated_words = generated_text[0]['generated_text'].split()[len(sentence.split()):]\n",
        "\n",
        "        # Extract the 7 most likely words\n",
        "        top_7_words = generated_words[:7]\n",
        "        results[f\"Sentence {i+1}\"] = top_7_words\n",
        "    return results\n",
        "\n",
        "\n",
        "p7_results = p_plus_7(sentences)\n",
        "\n",
        "for sentence_num, words in p7_results.items():\n",
        "    last_word = words[0] if words else \"No words generated\"\n",
        "    original_sentence = sentences[int(sentence_num.split()[1]) - 1]  # Get the original sentence\n",
        "    print(f\"{original_sentence} {last_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzRV9AJXXnuK",
        "outputId": "1f3b9d63-4437-407c-9f21-faeeefa9c44c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of what\n",
            "To regard the frost and the thaw\n",
            "Of the pine trees crusted with needles,\n",
            "And have been cold a long time,\n",
            "To behold the junipers shagged with their\n",
            "The spruces rough in the distant forest,\n",
            "Of the January sun; and not to the\n",
            "Of any misery in the sound of the war,\n",
            "In the sound of a few more\n",
            "Which is the sound of the rain...\n",
            "Full of the same the\n",
            "That is blowing in the same bare hands\n",
            "For the listener, who listens in the foreground,\n",
            "And, nothing himself, I\n",
            "Nothing that is not there and the nothing that is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzTkKvBUd_Ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
